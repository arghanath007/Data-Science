{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Embedding_using_Supervised_Learing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using Supervised Learning for Embedding of Words.\n",
        "\n",
        "> This is an old approach and is not that much popular. We are using it to predict wheather the food reviews are positive or negative. Hence, this is a Food Review Classification problem or an NLP problem in general."
      ],
      "metadata": {
        "id": "7PxI3r4V5fpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KViZltOY329a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = ['nice food',\n",
        "        'amazing restaurant',\n",
        "        'too good',\n",
        "        'just loved it!',\n",
        "        'will go again',\n",
        "        'horrible food',\n",
        "        'never go there',\n",
        "        'poor service',\n",
        "        'poor quality',\n",
        "        'needs improvement']\n",
        "\n",
        "# '1' means postive review and '0' means negative review\n",
        "sentiment = np.array([1,1,1,1,1,0,0,0,0,0]) "
      ],
      "metadata": {
        "id": "ohAvEct05AWy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the reviews\n",
        "\n",
        "> Using One Hot encoding to convert the reviews into feature vectors or one hot encoded vectors because machine can't understand words/strings."
      ],
      "metadata": {
        "id": "GcHitfm66JVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot('nice food',30) # '30' is the vocabulary size, so it will give unique numbers to the words between 1-30."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB03V8fYBAKZ",
        "outputId": "83c00bfb-5778-45f5-846b-a36582b27ea0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE=30\n",
        "\n",
        "encoded_reviews=[one_hot(review, VOCAB_SIZE) for review in reviews]\n",
        "encoded_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo7H0406B7_1",
        "outputId": "f7c35bd8-c526-45b6-a4c1-71e4ca0bbe7f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[13, 3],\n",
              " [5, 1],\n",
              " [23, 9],\n",
              " [29, 23, 20],\n",
              " [9, 11, 21],\n",
              " [22, 3],\n",
              " [7, 11, 20],\n",
              " [22, 15],\n",
              " [22, 28],\n",
              " [19, 29]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCUSK8bJCIwx",
        "outputId": "199e86ac-62bb-4390-b47a-68c32c558728"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(encoded_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWC0BnzMCOe-",
        "outputId": "54d21ee2-29d2-4050-bdc3-e3d7a6278e98"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH=np.argmax(encoded_reviews) +1\n",
        "MAX_LENGTH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1VOJHyOCimz",
        "outputId": "2078d72e-2f78-4d21-fb90-b5ed3184d400"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH=np.argmax(encoded_reviews) +1 \n",
        "padded_reviews= pad_sequences(encoded_reviews,maxlen=MAX_LENGTH, padding='post') # 'post' means towards the end\n",
        "padded_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmojMVn8CunG",
        "outputId": "bf9db177-0ec4-4b61-c28e-c8c214908ab8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13,  3,  0,  0],\n",
              "       [ 5,  1,  0,  0],\n",
              "       [23,  9,  0,  0],\n",
              "       [29, 23, 20,  0],\n",
              "       [ 9, 11, 21,  0],\n",
              "       [22,  3,  0,  0],\n",
              "       [ 7, 11, 20,  0],\n",
              "       [22, 15,  0,  0],\n",
              "       [22, 28,  0,  0],\n",
              "       [19, 29,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(padded_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5swyIIvEDBXh",
        "outputId": "943a8945-6d29-4702-bb99-355101bb36f5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDED_VECTOR_SIZE=5\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(VOCAB_SIZE,EMBEDED_VECTOR_SIZE, input_length=MAX_LENGTH, name='embedding'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid')) # 1 neuron sigmoid activation function"
      ],
      "metadata": {
        "id": "92dHQwXvDHSc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= padded_reviews\n",
        "y=sentiment"
      ],
      "metadata": {
        "id": "chuphX4UEuG8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mowYZ-jeF0sU",
        "outputId": "81644f4d-8066-4d71-cae7-55e7ca9ec76e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 5)              150       \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 171\n",
            "Trainable params: 171\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=50,verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHBu5y-2GDVr",
        "outputId": "30f496f3-ec16-4ef1-c3b8-7b68daa5b971"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7ea47ce8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy=model.evaluate(X,y)\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWuCrwNnGUPQ",
        "outputId": "0bc7a8b7-98a9-499f-ea27-15615abb6b29"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 277ms/step - loss: 0.6375 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings\n",
        "\n",
        "> The sentiment classification is a fake problem, we are more interested in word embeddings. While solving the sentiment classification problem, we got the word embeddings. Word embeddings are nothing but those parameters in the neural network."
      ],
      "metadata": {
        "id": "mVl_pPwMGbLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights=model.get_layer('embedding').get_weights()[0]\n",
        "len(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1JeV9-6HVa2",
        "outputId": "caa4da4f-dd89-4a1b-e15a-dd0e2145e504"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights[13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68vZyydiHwnb",
        "outputId": "4b518802-fafd-4bb5-a1b5-83c895bf6552"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.03869608, -0.06790911, -0.02366788, -0.00437879, -0.07389026],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6wCHa0yH6xQ",
        "outputId": "00fef653-19e1-4e19-f20b-35eb92a1b2f4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.07242334, -0.05629948, -0.02245453, -0.07807696, -0.05699418],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D0P0b1WbIDBD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}