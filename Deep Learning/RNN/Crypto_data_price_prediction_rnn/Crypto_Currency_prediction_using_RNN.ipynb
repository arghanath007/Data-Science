{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crypto_Currency_prediction_using_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Video(1)** ->https://www.youtube.com/watch?v=ne-dpRdNReI\n",
        "\n",
        "\n",
        "**Video(2)** -> https://www.youtube.com/watch?v=dvn6OAEi2G0\n",
        "\n",
        "**Video(3)** -> https://www.youtube.com/watch?v=jjmwM92xwq0\n",
        "\n",
        "**Video(4)** -> https://www.youtube.com/watch?v=yWkpRdpOiPY"
      ],
      "metadata": {
        "id": "R_KaE-2irkCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iYSvNIVzwJ0o"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "YNcBtZC6DP57"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/crypto_data/LTC-USD.csv',names=['time','low','high','open','close','volume'])"
      ],
      "metadata": {
        "id": "GBh4FSz2DfYR"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6RHSbZeNOad",
        "outputId": "dae58820-b76f-450f-9af5-0fdc4e0d8076"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         time        low       high       open      close      volume\n",
            "0  1528968660  96.580002  96.589996  96.589996  96.580002    9.647200\n",
            "1  1528968720  96.449997  96.669998  96.589996  96.660004  314.387024\n",
            "2  1528968780  96.470001  96.570000  96.570000  96.570000   77.129799\n",
            "3  1528968840  96.449997  96.570000  96.570000  96.500000    7.216067\n",
            "4  1528968900  96.279999  96.540001  96.500000  96.389999  524.539978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_df=pd.DataFrame()"
      ],
      "metadata": {
        "id": "FeLvwezpNRU4"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 60   # Last 60mins of pricing data to make a predictions.\n",
        "FUTURE_PERIOD_PREDICT= 3   # 'Period = 1min.\n",
        "RATIO_TO_PREDICT= \"LTC-USD\""
      ],
      "metadata": {
        "id": "b8sEpdL6RC29"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratios=[\"BTC-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"]"
      ],
      "metadata": {
        "id": "lPFqcl6RNfqM"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ratio in ratios:\n",
        "  dataset= f\"/content/drive/MyDrive/Colab Notebooks/crypto_data/{ratio}.csv\"\n",
        "\n",
        "  df=pd.read_csv(dataset, names=['time','low','high','open','close','volume'])\n",
        "  print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZgtglmUOZVZ",
        "outputId": "88053f91-e8e2-4af1-a3df-50c7f53c9c6d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         time          low         high         open        close    volume\n",
            "0  1528968660  6489.549805  6489.560059  6489.560059  6489.549805  0.587100\n",
            "1  1528968720  6487.370117  6489.560059  6489.549805  6487.379883  7.706374\n",
            "2  1528968780  6479.410156  6487.370117  6487.370117  6479.410156  3.088252\n",
            "3  1528968840  6479.410156  6479.419922  6479.419922  6479.410156  1.404100\n",
            "4  1528968900  6475.930176  6479.979980  6479.410156  6479.979980  0.753000\n",
            "         time        low       high       open      close      volume\n",
            "0  1528968660  96.580002  96.589996  96.589996  96.580002    9.647200\n",
            "1  1528968720  96.449997  96.669998  96.589996  96.660004  314.387024\n",
            "2  1528968780  96.470001  96.570000  96.570000  96.570000   77.129799\n",
            "3  1528968840  96.449997  96.570000  96.570000  96.500000    7.216067\n",
            "4  1528968900  96.279999  96.540001  96.500000  96.389999  524.539978\n",
            "         time        low   high        open      close     volume\n",
            "0  1528968720  485.98999  486.5  486.019989  486.01001  26.019083\n",
            "1  1528968780  486.00000  486.0  486.000000  486.00000   8.449400\n",
            "2  1528968840  485.75000  486.0  486.000000  485.75000  26.994646\n",
            "3  1528968900  485.75000  486.0  485.750000  486.00000  77.355759\n",
            "4  1528968960  485.98999  486.0  486.000000  486.00000   7.503300\n",
            "         time         low        high        open       close     volume\n",
            "0  1528968660  871.650024  871.729980  871.650024  871.719971   5.675361\n",
            "1  1528968720  870.859985  871.719971  871.719971  870.859985  26.856577\n",
            "2  1528968780  870.099976  871.090027  871.090027  870.099976   1.124300\n",
            "3  1528968840  868.830017  870.950012  868.830017  870.789978   1.749862\n",
            "4  1528968900  870.000000  870.000000  870.000000  870.000000   1.680500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ratio in ratios:\n",
        "  dataset= f\"/content/drive/MyDrive/Colab Notebooks/crypto_data/{ratio}.csv\"\n",
        "\n",
        "  df=pd.read_csv(dataset, names=['time','low','high','open','close','volume'])\n",
        "  df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "  df.set_index('time', inplace=True)\n",
        "  df=df[[f\"{ratio}_close\",f\"{ratio}_volume\"]]\n",
        "\n",
        "  if len(main_df) == 0: #empty\n",
        "    main_df=df\n",
        "\n",
        "  else:\n",
        "    main_df= main_df.join(df)\n",
        "\n",
        "print(main_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrWq1iw5OtEI",
        "outputId": "b097e4cb-aa7c-45c1-d13e-889d0d770948"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
            "time                                                                       \n",
            "1528968660    6489.549805        0.587100      96.580002        9.647200   \n",
            "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
            "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
            "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
            "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
            "...                   ...             ...            ...             ...   \n",
            "1535214960    6713.140137        0.769891      58.020000        6.434783   \n",
            "1535215020    6714.520020        1.002652      58.009998        7.301921   \n",
            "1535215080    6714.520020        1.021925      58.020000       23.802017   \n",
            "1535215140    6715.000000        3.645508      58.020000        6.953497   \n",
            "1535215200    6715.000000        0.513560      58.080002      202.403183   \n",
            "\n",
            "            ETH-USD_close  ETH-USD_volume  BCH-USD_close  BCH-USD_volume  \n",
            "time                                                                      \n",
            "1528968660            NaN             NaN     871.719971        5.675361  \n",
            "1528968720     486.010010       26.019083     870.859985       26.856577  \n",
            "1528968780     486.000000        8.449400     870.099976        1.124300  \n",
            "1528968840     485.750000       26.994646     870.789978        1.749862  \n",
            "1528968900     486.000000       77.355759     870.000000        1.680500  \n",
            "...                   ...             ...            ...             ...  \n",
            "1535214960     279.359985       11.280577     531.479980        1.208560  \n",
            "1535215020     279.359985        8.790519     531.479980        0.016868  \n",
            "1535215080     279.369995        1.311763     531.469971        0.013854  \n",
            "1535215140     279.660004       11.752819     531.479980        0.016900  \n",
            "1535215200     279.649994        8.351710     531.479980        0.299520  \n",
            "\n",
            "[97724 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Function\n",
        "\n",
        "> '1' means it is a good thing. We should buy it. '0' means it is not a good thing. Do not buy.\n",
        "\n",
        ">We want to train the model based on these sequence of features, price goes up and with other sequence of features price goes down."
      ],
      "metadata": {
        "id": "yNfzJyFESteA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(current, future):\n",
        "  if float(future) > float(current):\n",
        "    return 1\n",
        "\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "1cYEPFvPQCR_"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future Price"
      ],
      "metadata": {
        "id": "8rJ4zmO6VSjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df['future']= main_df[f\"{RATIO_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
        "\n",
        "print(main_df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79jq_KuLSsPl",
        "outputId": "8fc69764-6076-4153-90ab-b24dfc4f3758"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
            "time                                                                       \n",
            "1528968660    6489.549805        0.587100      96.580002        9.647200   \n",
            "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
            "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
            "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
            "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
            "1528968960    6480.000000        1.490900      96.519997       16.991997   \n",
            "1528969020    6477.220215        2.731950      96.440002       95.524078   \n",
            "1528969080    6480.000000        2.174240      96.470001      175.205307   \n",
            "1528969140    6479.990234        0.903100      96.400002       43.652802   \n",
            "1528969200    6478.660156        3.258786      96.400002        8.160000   \n",
            "\n",
            "            ETH-USD_close  ETH-USD_volume  BCH-USD_close  BCH-USD_volume  \\\n",
            "time                                                                       \n",
            "1528968660            NaN             NaN     871.719971        5.675361   \n",
            "1528968720      486.01001       26.019083     870.859985       26.856577   \n",
            "1528968780      486.00000        8.449400     870.099976        1.124300   \n",
            "1528968840      485.75000       26.994646     870.789978        1.749862   \n",
            "1528968900      486.00000       77.355759     870.000000        1.680500   \n",
            "1528968960      486.00000        7.503300     869.989990        1.669014   \n",
            "1528969020      485.98999       85.877251     869.450012        0.865200   \n",
            "1528969080      485.98999      160.915192     869.989990       23.534929   \n",
            "1528969140      485.98999       61.371887     870.000000        2.300000   \n",
            "1528969200      485.98999       42.687656     870.320007        9.255514   \n",
            "\n",
            "               future  \n",
            "time                   \n",
            "1528968660  96.500000  \n",
            "1528968720  96.389999  \n",
            "1528968780  96.519997  \n",
            "1528968840  96.440002  \n",
            "1528968900  96.470001  \n",
            "1528968960  96.400002  \n",
            "1528969020  96.400002  \n",
            "1528969080  96.400002  \n",
            "1528969140  96.400002  \n",
            "1528969200  96.400002  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target\n",
        "\n",
        "> 'main_df[f\"{RATIO_TO_PREDICT}_close\"]' is the current price\n",
        "\n",
        "> 'main_df['future']' is the future price.\n",
        "\n",
        "> If the future price is more/greater than the current price then return '1' in the target which means it is a good thing so buy. \n",
        "\n",
        ">Other return '0' which means do not buy."
      ],
      "metadata": {
        "id": "0qjI_zhaWBOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df['target']= list(map(classify, main_df[f\"{RATIO_TO_PREDICT}_close\"], main_df['future']))\n",
        "\n",
        "print(main_df[[f\"{RATIO_TO_PREDICT}_close\",'future','target']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1752WeMpU_Ty",
        "outputId": "a095a3b3-2521-45b4-8cde-8026e4797641"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            LTC-USD_close     future  target\n",
            "time                                        \n",
            "1528968660      96.580002  96.500000       0\n",
            "1528968720      96.660004  96.389999       0\n",
            "1528968780      96.570000  96.519997       0\n",
            "1528968840      96.500000  96.440002       0\n",
            "1528968900      96.389999  96.470001       1\n",
            "1528968960      96.519997  96.400002       0\n",
            "1528969020      96.440002  96.400002       0\n",
            "1528969080      96.470001  96.400002       0\n",
            "1528969140      96.400002  96.400002       0\n",
            "1528969200      96.400002  96.400002       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6gsPuLrjWsGi",
        "outputId": "d28f5e99-d639-48f9-e5fe-0df9133f946e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
              "time                                                                       \n",
              "1528968660    6489.549805        0.587100      96.580002        9.647200   \n",
              "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
              "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
              "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
              "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
              "1528968960    6480.000000        1.490900      96.519997       16.991997   \n",
              "1528969020    6477.220215        2.731950      96.440002       95.524078   \n",
              "1528969080    6480.000000        2.174240      96.470001      175.205307   \n",
              "1528969140    6479.990234        0.903100      96.400002       43.652802   \n",
              "1528969200    6478.660156        3.258786      96.400002        8.160000   \n",
              "\n",
              "            ETH-USD_close  ETH-USD_volume  BCH-USD_close  BCH-USD_volume  \\\n",
              "time                                                                       \n",
              "1528968660            NaN             NaN     871.719971        5.675361   \n",
              "1528968720      486.01001       26.019083     870.859985       26.856577   \n",
              "1528968780      486.00000        8.449400     870.099976        1.124300   \n",
              "1528968840      485.75000       26.994646     870.789978        1.749862   \n",
              "1528968900      486.00000       77.355759     870.000000        1.680500   \n",
              "1528968960      486.00000        7.503300     869.989990        1.669014   \n",
              "1528969020      485.98999       85.877251     869.450012        0.865200   \n",
              "1528969080      485.98999      160.915192     869.989990       23.534929   \n",
              "1528969140      485.98999       61.371887     870.000000        2.300000   \n",
              "1528969200      485.98999       42.687656     870.320007        9.255514   \n",
              "\n",
              "               future  target  \n",
              "time                           \n",
              "1528968660  96.500000       0  \n",
              "1528968720  96.389999       0  \n",
              "1528968780  96.519997       0  \n",
              "1528968840  96.440002       0  \n",
              "1528968900  96.470001       1  \n",
              "1528968960  96.400002       0  \n",
              "1528969020  96.400002       0  \n",
              "1528969080  96.400002       0  \n",
              "1528969140  96.400002       0  \n",
              "1528969200  96.400002       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7d88fb9-10f3-476a-a60c-0b5da0deeda3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BTC-USD_close</th>\n",
              "      <th>BTC-USD_volume</th>\n",
              "      <th>LTC-USD_close</th>\n",
              "      <th>LTC-USD_volume</th>\n",
              "      <th>ETH-USD_close</th>\n",
              "      <th>ETH-USD_volume</th>\n",
              "      <th>BCH-USD_close</th>\n",
              "      <th>BCH-USD_volume</th>\n",
              "      <th>future</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1528968660</th>\n",
              "      <td>6489.549805</td>\n",
              "      <td>0.587100</td>\n",
              "      <td>96.580002</td>\n",
              "      <td>9.647200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>871.719971</td>\n",
              "      <td>5.675361</td>\n",
              "      <td>96.500000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528968720</th>\n",
              "      <td>6487.379883</td>\n",
              "      <td>7.706374</td>\n",
              "      <td>96.660004</td>\n",
              "      <td>314.387024</td>\n",
              "      <td>486.01001</td>\n",
              "      <td>26.019083</td>\n",
              "      <td>870.859985</td>\n",
              "      <td>26.856577</td>\n",
              "      <td>96.389999</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528968780</th>\n",
              "      <td>6479.410156</td>\n",
              "      <td>3.088252</td>\n",
              "      <td>96.570000</td>\n",
              "      <td>77.129799</td>\n",
              "      <td>486.00000</td>\n",
              "      <td>8.449400</td>\n",
              "      <td>870.099976</td>\n",
              "      <td>1.124300</td>\n",
              "      <td>96.519997</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528968840</th>\n",
              "      <td>6479.410156</td>\n",
              "      <td>1.404100</td>\n",
              "      <td>96.500000</td>\n",
              "      <td>7.216067</td>\n",
              "      <td>485.75000</td>\n",
              "      <td>26.994646</td>\n",
              "      <td>870.789978</td>\n",
              "      <td>1.749862</td>\n",
              "      <td>96.440002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528968900</th>\n",
              "      <td>6479.979980</td>\n",
              "      <td>0.753000</td>\n",
              "      <td>96.389999</td>\n",
              "      <td>524.539978</td>\n",
              "      <td>486.00000</td>\n",
              "      <td>77.355759</td>\n",
              "      <td>870.000000</td>\n",
              "      <td>1.680500</td>\n",
              "      <td>96.470001</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528968960</th>\n",
              "      <td>6480.000000</td>\n",
              "      <td>1.490900</td>\n",
              "      <td>96.519997</td>\n",
              "      <td>16.991997</td>\n",
              "      <td>486.00000</td>\n",
              "      <td>7.503300</td>\n",
              "      <td>869.989990</td>\n",
              "      <td>1.669014</td>\n",
              "      <td>96.400002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528969020</th>\n",
              "      <td>6477.220215</td>\n",
              "      <td>2.731950</td>\n",
              "      <td>96.440002</td>\n",
              "      <td>95.524078</td>\n",
              "      <td>485.98999</td>\n",
              "      <td>85.877251</td>\n",
              "      <td>869.450012</td>\n",
              "      <td>0.865200</td>\n",
              "      <td>96.400002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528969080</th>\n",
              "      <td>6480.000000</td>\n",
              "      <td>2.174240</td>\n",
              "      <td>96.470001</td>\n",
              "      <td>175.205307</td>\n",
              "      <td>485.98999</td>\n",
              "      <td>160.915192</td>\n",
              "      <td>869.989990</td>\n",
              "      <td>23.534929</td>\n",
              "      <td>96.400002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528969140</th>\n",
              "      <td>6479.990234</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>96.400002</td>\n",
              "      <td>43.652802</td>\n",
              "      <td>485.98999</td>\n",
              "      <td>61.371887</td>\n",
              "      <td>870.000000</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>96.400002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528969200</th>\n",
              "      <td>6478.660156</td>\n",
              "      <td>3.258786</td>\n",
              "      <td>96.400002</td>\n",
              "      <td>8.160000</td>\n",
              "      <td>485.98999</td>\n",
              "      <td>42.687656</td>\n",
              "      <td>870.320007</td>\n",
              "      <td>9.255514</td>\n",
              "      <td>96.400002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7d88fb9-10f3-476a-a60c-0b5da0deeda3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7d88fb9-10f3-476a-a60c-0b5da0deeda3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7d88fb9-10f3-476a-a60c-0b5da0deeda3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "times= sorted(main_df.index.values)\n",
        "\n",
        "last_five_percent= times[-int(0.05 * len(times))]\n",
        "print(last_five_percent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmnW54NsW5Uu",
        "outputId": "26832847-89f2-4dab-87a6-68a2c8eb1316"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1534922100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_main_df = main_df[(main_df.index >= last_five_percent)]\n",
        "main_df= main_df[(main_df.index < last_five_percent)]"
      ],
      "metadata": {
        "id": "rpZi-V5ovAr7"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Function"
      ],
      "metadata": {
        "id": "wsgTcJVDwBa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_df(df):\n",
        "#   df= df.drop(\"future\", 1)\n",
        "\n",
        "#   for col in df.columns:\n",
        "#     if col != \"target\":\n",
        "#       df[col]= df[col].pct_change() # 'pct_change()' -> Normalizes all of the data\n",
        "#       df.dropna(inplace=True)\n",
        "#       df[col]= preprocessing.scale(df[col].values) # Scaling the data between 0-1\n",
        "\n",
        "\n",
        "#   df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "#   sequential_data=[]\n",
        "#   prev_days= deque(maxlen=SEQ_LEN)\n",
        "#   # print(df.head())\n",
        "#   for i in df.values: # Converts DataFrame into Lists of Lists. So it won't contain time anymore but it will be in the order of the index still. 'i' is a row of columns.\n",
        "\n",
        "#     prev_days.append([n for n in i[:-1]]) # Excluding 'target' column\n",
        "\n",
        "#     if len(prev_days) == SEQ_LEN:\n",
        "#       sequential_data.append([np.array(prev_days), i[:-1]])\n",
        "      \n",
        "\n",
        "    \n",
        "#   random.shuffle(sequential_data) # Build the Sequential data.\n",
        "\n",
        "\n",
        "#   # Balancing the Data.\n",
        "\n",
        "#   buys=[]\n",
        "#   sells=[]\n",
        "\n",
        "#   for seq, target in sequential_data:\n",
        "#     if target == 0: # if it's a \"not buy\"\n",
        "#       sells.append([seq, target])\n",
        "\n",
        "#     elif target == 1: # otherwise if the target is a 1\n",
        "#       buys.append([seq, target])\n",
        "\n",
        "\n",
        "#   random.shuffle(buys)\n",
        "#   random.shuffle(sells)\n",
        "\n",
        "\n",
        "#   lower= min(len(buys), len(sells))\n",
        "#   buys= buys[:lower]\n",
        "#   sells= sells[:lower]\n",
        "\n",
        "#   sequential_data = buys + sells\n",
        "\n",
        "#   random.shuffle(sequential_data)\n",
        "\n",
        "\n",
        "#   X=[]\n",
        "#   y=[]\n",
        "\n",
        "#   for seq, target in sequential_data:\n",
        "#     X.append(seq)\n",
        "#     y.append(target)\n",
        "\n",
        "#   return np.array(X), y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic.\n",
        "\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "    buys = []  # list that will store our buy sequences and targets\n",
        "    sells = []  # list that will store our sell sequences and targets\n",
        "\n",
        "    for seq, target in sequential_data:  # iterate over the sequential data\n",
        "        if target == 0:  # if it's a \"not buy\"\n",
        "            sells.append([seq, target])  # append to sells list\n",
        "        elif target == 1:  # otherwise if the target is a 1...\n",
        "            buys.append([seq, target])  # it's a buy!\n",
        "\n",
        "    random.shuffle(buys)  # shuffle the buys\n",
        "    random.shuffle(sells)  # shuffle the sells!\n",
        "\n",
        "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
        "\n",
        "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
        "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
        "\n",
        "    sequential_data = buys+sells  # add them together\n",
        "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:  # going over our new sequential data\n",
        "        X.append(seq)  # X is the sequences\n",
        "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
        "\n",
        "    return np.array(X), y  # return X and y...and make X a numpy array!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-81o4BMcvwyV"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df['target']= list(map(classify, main_df[f\"{RATIO_TO_PREDICT}_close\"], main_df['future']))\n",
        "\n",
        "print(main_df[[f\"{RATIO_TO_PREDICT}_close\",'future','target']].head(10))\n",
        "\n",
        "times= sorted(main_df.index.values)\n",
        "\n",
        "last_five_percent= times[-int(0.05 * len(times))]\n",
        "print(last_five_percent)\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_five_percent)]\n",
        "main_df= main_df[(main_df.index < last_five_percent)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "xEZske7JviFw",
        "outputId": "499a9b48-a34f-4bea-b336-46a835c984f3"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'future'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-05d3e0e65be4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{RATIO_TO_PREDICT}_close\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'future'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{RATIO_TO_PREDICT}_close\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'future'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'future'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess_df(main_df)"
      ],
      "metadata": {
        "id": "VvEDC748wedz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balancing the data\n",
        "\n",
        "> (50/50 split is ideal) but 48/52 is also fine but if the split is like 40/60 then we should balance the dataset.\n",
        "\n",
        "> The downfall for not balancing the dataset is that the model will learn pretty quickly that always predict that one class(the class with a higher split in the dataset) and immediately we will make huge improvements and that's the easiest change that the model can make.\n",
        "\n",
        "> So it is important that we have a perfect/ close to perfect split and balance of data, so that the model doesn't waste time doing the above mistake."
      ],
      "metadata": {
        "id": "kHVXfL5hB1tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# buys=[]\n",
        "# sells=[]\n",
        "\n",
        "# for seq, target in sequential_data:\n",
        "#   if target ==0:\n",
        "#     sells.append([seq, target])\n",
        "\n",
        "#   elif target == 1:\n",
        "#     buys.append([seq, target])\n",
        "\n",
        "\n",
        "# random.shuffle(buys)\n",
        "# random.shuffle(sells)\n",
        "\n",
        "\n",
        "# lower= min(len(buys), len(sells))\n",
        "# buys= buys[:lower]\n",
        "# sells= sells[:lower]\n",
        "\n",
        "# sequential_data = buys + sells\n",
        "\n",
        "# random.shuffle(sequential_data)\n",
        "\n",
        "\n",
        "# X=[]\n",
        "# y=[]\n",
        "\n",
        "# for seq, target in sequential_data:\n",
        "#   X.append(seq)\n",
        "#   y.append(target)\n",
        "\n",
        "# return np.array(X), y\n"
      ],
      "metadata": {
        "id": "U_rs5-7dha_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main_df['target']= list(map(classify, main_df[f\"{RATIO_TO_PREDICT}_close\"], main_df['future']))\n",
        "\n",
        "# print(main_df[[f\"{RATIO_TO_PREDICT}_close\",'future','target']].head(10))\n",
        "\n",
        "# times= sorted(main_df.index.values)\n",
        "\n",
        "# last_five_percent= times[-int(0.05 * len(times))]\n",
        "# print(last_five_percent)\n",
        "\n",
        "# validation_main_df = main_df[(main_df.index >= last_five_percent)]\n",
        "# main_df= main_df[(main_df.index < last_five_percent)]"
      ],
      "metadata": {
        "id": "r07WbnP6jyfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)"
      ],
      "metadata": {
        "id": "z-ljEJl3lKr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df.head()"
      ],
      "metadata": {
        "id": "7Da46P3HmuFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "BuMjvijVm8WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequential_data=[]\n",
        "prev_days=deque(maxlen=SEQ_LEN)\n",
        "# print(df.head())\n",
        "for i in df.values: # Converts DataFrame into Lists of Lists. So it won't contain time anymore but it will be in the order of the index still. 'i' is a row of columns.\n",
        "\n",
        "  prev_days.append([n for n in i[:-1]]) # Excluding 'target' column\n",
        "\n",
        "  if len(prev_days) == SEQ_LEN:\n",
        "    sequential_data.append([np.array(prev_days), i[:-1]])\n",
        "    \n",
        "\n",
        "  \n",
        "random.shuffle(sequential_data) # Build the Sequential data.\n",
        "print(sequential_data)"
      ],
      "metadata": {
        "id": "NXxXq94Xm-LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "    print(ratio)\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  # read in specific file\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "print(main_df.head())\n"
      ],
      "metadata": {
        "id": "FeJRZUusrp5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from sklearn import preprocessing  # pip install sklearn ... if you don't have it!\n",
        "\n",
        "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
        "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\n",
        "\n",
        "\n",
        "def classify(current, future):\n",
        "    if float(future) > float(current):  # if the future price is higher than the current, that's a buy, or a 1\n",
        "        return 1\n",
        "    else:  # otherwise... it's a 0!\n",
        "        return 0\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic. Those nasty NaNs love to creep in.\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "    print(ratio)\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  # read in specific file\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "#print(main_df.head())\n",
        "\n",
        "times = sorted(main_df.index.values)  # get the times\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
        "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%\n",
        "    buys = []  # list that will store our buy sequences and targets\n",
        "    sells = []  # list that will store our sell sequences and targets\n",
        "\n",
        "    for seq, target in sequential_data:  # iterate over the sequential data\n",
        "        if target == 0:  # if it's a \"not buy\"\n",
        "            sells.append([seq, target])  # append to sells list\n",
        "        elif target == 1:  # otherwise if the target is a 1...\n",
        "            buys.append([seq, target])  # it's a buy!\n",
        "\n",
        "    random.shuffle(buys)  # shuffle the buys\n",
        "    random.shuffle(sells)  # shuffle the sells!\n",
        "\n",
        "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
        "\n",
        "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
        "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
        "\n",
        "    sequential_data = buys+sells  # add them together\n",
        "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
        "\n",
        "Now, all we need to do is split the data back to featuresets and labels/targets. Easy enough:\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:  # going over our new sequential data\n",
        "        X.append(seq)  # X is the sequences\n",
        "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
        "\n",
        "    return np.array(X), y  # return X and y...and make X a numpy array! ..import numpy as np\n",
        "\n",
        "Full function now:\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic.\n",
        "\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "    buys = []  # list that will store our buy sequences and targets\n",
        "    sells = []  # list that will store our sell sequences and targets\n",
        "\n",
        "    for seq, target in sequential_data:  # iterate over the sequential data\n",
        "        if target == 0:  # if it's a \"not buy\"\n",
        "            sells.append([seq, target])  # append to sells list\n",
        "        elif target == 1:  # otherwise if the target is a 1...\n",
        "            buys.append([seq, target])  # it's a buy!\n",
        "\n",
        "    random.shuffle(buys)  # shuffle the buys\n",
        "    random.shuffle(sells)  # shuffle the sells!\n",
        "\n",
        "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
        "\n",
        "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
        "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
        "\n",
        "    sequential_data = buys+sells  # add them together\n",
        "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:  # going over our new sequential data\n",
        "        X.append(seq)  # X is the sequences\n",
        "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
        "\n",
        "    return np.array(X), y  # return X and y...and make X a numpy array!\n",
        "\n",
        "Full code up to this point:\n",
        "\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
        "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\n",
        "\n",
        "\n",
        "def classify(current, future):\n",
        "    if float(future) > float(current):  # if the future price is higher than the current, that's a buy, or a 1\n",
        "        return 1\n",
        "    else:  # otherwise... it's a 0!\n",
        "        return 0\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic. Those nasty NaNs love to creep in.\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "    print(ratio)\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  # read in specific file\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "#print(main_df.head())\n",
        "\n",
        "times = sorted(main_df.index.values)  # get the times\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
        "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic.\n",
        "\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "    buys = []  # list that will store our buy sequences and targets\n",
        "    sells = []  # list that will store our sell sequences and targets\n",
        "\n",
        "    for seq, target in sequential_data:  # iterate over the sequential data\n",
        "        if target == 0:  # if it's a \"not buy\"\n",
        "            sells.append([seq, target])  # append to sells list\n",
        "        elif target == 1:  # otherwise if the target is a 1...\n",
        "            buys.append([seq, target])  # it's a buy!\n",
        "\n",
        "    random.shuffle(buys)  # shuffle the buys\n",
        "    random.shuffle(sells)  # shuffle the sells!\n",
        "\n",
        "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
        "\n",
        "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
        "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
        "\n",
        "    sequential_data = buys+sells  # add them together\n",
        "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:  # going over our new sequential data\n",
        "        X.append(seq)  # X is the sequences\n",
        "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
        "\n",
        "    return np.array(X), y  # return X and y...and make X a numpy array!\n",
        "\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "\n",
        "    ratio = ratio.split('.csv')[0]  # split away the ticker from the file-name\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df = pd.read_csv(dataset, names=['time', 'low', 'high', 'open', 'close', 'volume'])  # read in specific file\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "main_df.dropna(inplace=True)\n",
        "\n",
        "## here, split away some slice of the future data from the main main_df.\n",
        "times = sorted(main_df.index.values)\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
        "main_df = main_df[(main_df.index < last_5pct)]\n",
        "\n",
        "\n",
        "\n",
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "\n",
        "\n",
        "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "vnFBdSkm7EmU",
        "outputId": "8e913fb7-17de-4e22-dad0-4ee3f4aed6ad"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-100-4e55e9a22d9d>\"\u001b[0;36m, line \u001b[0;32m78\u001b[0m\n\u001b[0;31m    Continuing along in our preprocess_df function, we can balance by doing:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "\n",
        "\n",
        "# from keras.layers import Input, LSTM, Dense, TimeDistributed, Activation, BatchNormalization, Dropout, Bidirectional\n",
        "# from keras.models import Sequential\n",
        "# from keras.utils import Sequence\n",
        "# from keras.layers import CuDNNLSTM\n",
        "\n",
        "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
        "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\n",
        "EPOCHS=10\n",
        "BATCH_SIZE=64\n",
        "NAME= f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
        "\n",
        "\n",
        "def classify(current, future):\n",
        "    if float(future) > float(current):  # if the future price is higher than the current, that's a buy, or a 1\n",
        "        return 1\n",
        "    else:  # otherwise... it's a 0!\n",
        "        return 0\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic. Those nasty NaNs love to creep in.\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "    print(ratio)\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/crypto_data/LTC-USD.csv',names=['time','low','high','open','close','volume'])\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "#print(main_df.head())\n",
        "\n",
        "times = sorted(main_df.index.values)  # get the times\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
        "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic.\n",
        "\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "    buys = []  # list that will store our buy sequences and targets\n",
        "    sells = []  # list that will store our sell sequences and targets\n",
        "\n",
        "    for seq, target in sequential_data:  # iterate over the sequential data\n",
        "        if target == 0:  # if it's a \"not buy\"\n",
        "            sells.append([seq, target])  # append to sells list\n",
        "        elif target == 1:  # otherwise if the target is a 1...\n",
        "            buys.append([seq, target])  # it's a buy!\n",
        "\n",
        "    random.shuffle(buys)  # shuffle the buys\n",
        "    random.shuffle(sells)  # shuffle the sells!\n",
        "\n",
        "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
        "\n",
        "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
        "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
        "\n",
        "    sequential_data = buys+sells  # add them together\n",
        "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:  # going over our new sequential data\n",
        "        X.append(seq)  # X is the sequences\n",
        "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
        "\n",
        "    return np.array(X), y  # return X and y...and make X a numpy array!\n",
        "\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "\n",
        "    ratio = ratio.split('.csv')[0]  # split away the ticker from the file-name\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/crypto_data/LTC-USD.csv',names=['time','low','high','open','close','volume'])\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "main_df.dropna(inplace=True)\n",
        "\n",
        "## here, split away some slice of the future data from the main main_df.\n",
        "times = sorted(main_df.index.values)\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
        "main_df = main_df[(main_df.index < last_5pct)]\n",
        "\n",
        "\n",
        "\n",
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "\n",
        "\n",
        "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "# Compile model\n",
        "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# TensorBoard callback:\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
        "\n",
        "# ModelCheckpoint callback\n",
        "\n",
        "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
        "checkpoint = ModelCheckpoint(\"crypto_models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
        "\n",
        "\n",
        "train_x = np.asarray(train_x)\n",
        "train_y = np.asarray(train_y)\n",
        "validation_x = np.asarray(validation_x)\n",
        "validation_y = np.asarray(validation_y)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(validation_x, validation_y),\n",
        "    callbacks=[tensorboard],\n",
        "    # callbacks=[tensorboard, checkpoint],\n",
        ")\n",
        "\n",
        "\n",
        "# Score model\n",
        "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save model\n",
        "model.save(\"crypto_models/{}\".format(NAME))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyiVmrji7FJS",
        "outputId": "51590d42-4689-4c10-bd77-7b9aeeb41ff8"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTC-USD\n",
            "LTC-USD\n",
            "BCH-USD\n",
            "ETH-USD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data: 81638 validation: 4022\n",
            "Dont buys: 40819, buys: 40819\n",
            "VALIDATION Dont buys: 2011, buys: 2011\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1276/1276 [==============================] - 27s 18ms/step - loss: 0.7138 - accuracy: 0.5062 - val_loss: 0.6931 - val_accuracy: 0.5022\n",
            "Epoch 2/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6939 - accuracy: 0.5010 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6934 - accuracy: 0.5034 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "1276/1276 [==============================] - 23s 18ms/step - loss: 0.6934 - accuracy: 0.5029 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "1276/1276 [==============================] - 22s 18ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6932 - val_accuracy: 0.5127\n",
            "Epoch 6/10\n",
            "1276/1276 [==============================] - 22s 18ms/step - loss: 0.6932 - accuracy: 0.5083 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6932 - accuracy: 0.5011 - val_loss: 0.6940 - val_accuracy: 0.4973\n",
            "Epoch 8/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6930 - val_accuracy: 0.5129\n",
            "Epoch 9/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6930 - accuracy: 0.5041 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6915 - accuracy: 0.5254 - val_loss: 0.6902 - val_accuracy: 0.5229\n",
            "Test loss: 0.6901781558990479\n",
            "Test accuracy: 0.5228741765022278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: crypto_models/60-SEQ-3-PRED-1655191943/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: crypto_models/60-SEQ-3-PRED-1655191943/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f4725d36fd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f46bbd16150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f46bb45db90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "es6rbSgEJTA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=logs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wK0t2SQ_KFl",
        "outputId": "fded81ca-a2e1-4c11-e5ce-6c13c9ca879f"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/util.py\", line 320, in _exit_function\n",
            "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "5VEIL36rDkFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}