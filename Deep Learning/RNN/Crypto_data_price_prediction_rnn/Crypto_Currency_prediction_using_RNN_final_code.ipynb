{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crypto_Currency_prediction_using_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Video(1)** ->https://www.youtube.com/watch?v=ne-dpRdNReI\n",
        "\n",
        "\n",
        "**Video(2)** -> https://www.youtube.com/watch?v=dvn6OAEi2G0\n",
        "\n",
        "**Video(3)** -> https://www.youtube.com/watch?v=jjmwM92xwq0\n",
        "\n",
        "**Video(4)** -> https://www.youtube.com/watch?v=yWkpRdpOiPY"
      ],
      "metadata": {
        "id": "R_KaE-2irkCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
      ],
      "metadata": {
        "id": "qllqpy7KPHUE"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN. Last 60mins of pricing data to make a predictions.\n",
        "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict? 'Period' = 1min.\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\n",
        "EPOCHS=10\n",
        "BATCH_SIZE=64\n",
        "NAME= f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
      ],
      "metadata": {
        "id": "jk551kmjPawY"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Function\n",
        "\n",
        "> '1' means it is a good thing. We should buy it. '0' means it is not a good thing. Do not buy.\n",
        "\n",
        ">We want to train the model based on these sequence of features, price goes up and with other sequence of features price goes down."
      ],
      "metadata": {
        "id": "pWUC7nl2R-cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(current, future):\n",
        "    if float(future) > float(current):  # if the future price is higher than the current, that's a buy, or a 1\n",
        "        return 1\n",
        "    else:  # otherwise... it's a 0!\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "lmnLY7T9PeKJ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vslx1792Sa4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Function"
      ],
      "metadata": {
        "id": "yxtj-QZ7SMUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic.\n",
        "\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "    buys = []  # list that will store our buy sequences and targets\n",
        "    sells = []  # list that will store our sell sequences and targets\n",
        "\n",
        "    for seq, target in sequential_data:  # iterate over the sequential data\n",
        "        if target == 0:  # if it's a \"not buy\"\n",
        "            sells.append([seq, target])  # append to sells list\n",
        "        elif target == 1:  # otherwise if the target is a 1...\n",
        "            buys.append([seq, target])  # it's a buy!\n",
        "\n",
        "    random.shuffle(buys)  # shuffle the buys\n",
        "    random.shuffle(sells)  # shuffle the sells!\n",
        "\n",
        "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
        "\n",
        "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
        "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
        "\n",
        "    sequential_data = buys+sells  # add them together\n",
        "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:  # going over our new sequential data\n",
        "        X.append(seq)  # X is the sequences\n",
        "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
        "\n",
        "    return np.array(X), y  # return X and y...and make X a numpy array!"
      ],
      "metadata": {
        "id": "qft78R_mPhxZ"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target\n",
        "\n",
        "> 'main_df[f\"{RATIO_TO_PREDICT}_close\"]' is the current price\n",
        "\n",
        "> 'main_df['future']' is the future price.\n",
        "\n",
        "> If the future price is more/greater than the current price then return '1' in the target which means it is a good thing so buy. \n",
        "\n",
        ">Other return '0' which means do not buy."
      ],
      "metadata": {
        "id": "LGooz27KSIH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "    print(ratio)\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/crypto_data/LTC-USD.csv',names=['time','low','high','open','close','volume'])\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "main_df.dropna(inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXvaocdaPrFi",
        "outputId": "ee40aefc-77ec-4d06-a3e6-c0d4d6c206d3"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTC-USD\n",
            "LTC-USD\n",
            "BCH-USD\n",
            "ETH-USD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balancing the data\n",
        "\n",
        "> (50/50 split is ideal) but 48/52 is also fine but if the split is like 40/60 then we should balance the dataset.\n",
        "\n",
        "> The downfall for not balancing the dataset is that the model will learn pretty quickly that always predict that one class(the class with a higher split in the dataset) and immediately we will make huge improvements and that's the easiest change that the model can make.\n",
        "\n",
        "> So it is important that we have a perfect/ close to perfect split and balance of data, so that the model doesn't waste time doing the above mistake."
      ],
      "metadata": {
        "id": "Ohe1UMUkQvKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# buys=[]\n",
        "# sells=[]\n",
        "\n",
        "# for seq, target in sequential_data:\n",
        "#   if target ==0:\n",
        "#     sells.append([seq, target])\n",
        "\n",
        "#   elif target == 1:\n",
        "#     buys.append([seq, target])\n",
        "\n",
        "\n",
        "# random.shuffle(buys)\n",
        "# random.shuffle(sells)\n",
        "\n",
        "\n",
        "# lower= min(len(buys), len(sells))\n",
        "# buys= buys[:lower]\n",
        "# sells= sells[:lower]\n",
        "\n",
        "# sequential_data = buys + sells\n",
        "\n",
        "# random.shuffle(sequential_data)\n",
        "\n",
        "\n",
        "# X=[]\n",
        "# y=[]\n",
        "\n",
        "# for seq, target in sequential_data:\n",
        "#   X.append(seq)\n",
        "#   y.append(target)\n",
        "\n",
        "# return np.array(X), y"
      ],
      "metadata": {
        "id": "ZE9O0ny6Sqf8"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "times = sorted(main_df.index.values)  # get the times\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
        "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%"
      ],
      "metadata": {
        "id": "-8M4PZC5QQlb"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtvPZfQpQyxK",
        "outputId": "145e14de-45d0-4a70-dd22-b0e4b2743e23"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6bldYQFQ0Eg",
        "outputId": "4b30b689-445f-442a-80eb-6600a164b662"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data: 81638 validation: 4022\n",
            "Dont buys: 40819, buys: 40819\n",
            "VALIDATION Dont buys: 2011, buys: 2011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "kZ2Fgm6YQ3CC"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
        "\n",
        "# ModelCheckpoint callback\n",
        "\n",
        "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
        "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
        "\n",
        "\n",
        "train_x = np.asarray(train_x)\n",
        "train_y = np.asarray(train_y)\n",
        "validation_x = np.asarray(validation_x)\n",
        "validation_y = np.asarray(validation_y)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(validation_x, validation_y),\n",
        "    callbacks=[tensorboard],\n",
        "    # callbacks=[tensorboard, checkpoint],\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwAfElR4Q6Uz",
        "outputId": "d9cb6881-e558-4ee5-b422-07f4f3cf710f"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1276/1276 [==============================] - 28s 18ms/step - loss: 0.7011 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6927 - accuracy: 0.5149 - val_loss: 0.6931 - val_accuracy: 0.4990\n",
            "Epoch 3/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6913 - accuracy: 0.5274 - val_loss: 0.6953 - val_accuracy: 0.5221\n",
            "Epoch 4/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6913 - accuracy: 0.5270 - val_loss: 0.6905 - val_accuracy: 0.5278\n",
            "Epoch 5/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6899 - accuracy: 0.5300 - val_loss: 0.6886 - val_accuracy: 0.5236\n",
            "Epoch 6/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6893 - accuracy: 0.5326 - val_loss: 0.6858 - val_accuracy: 0.5552\n",
            "Epoch 7/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6870 - accuracy: 0.5440 - val_loss: 0.6842 - val_accuracy: 0.5475\n",
            "Epoch 8/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6850 - accuracy: 0.5472 - val_loss: 0.6847 - val_accuracy: 0.5455\n",
            "Epoch 9/10\n",
            "1276/1276 [==============================] - 21s 17ms/step - loss: 0.6849 - accuracy: 0.5472 - val_loss: 0.6869 - val_accuracy: 0.5425\n",
            "Epoch 10/10\n",
            "1276/1276 [==============================] - 21s 17ms/step - loss: 0.6834 - accuracy: 0.5531 - val_loss: 0.6777 - val_accuracy: 0.5674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Score model\n",
        "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save model\n",
        "model.save(\"crypto_models/{}\".format(NAME))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkzsIQ14RfSC",
        "outputId": "3cf6ce6c-7cb3-4d38-ecc4-252826a603c7"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6777359247207642\n",
            "Test accuracy: 0.5673794150352478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: crypto_models/60-SEQ-3-PRED-1655193850/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: crypto_models/60-SEQ-3-PRED-1655193850/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f463730ef90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f463782f350> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f46bae038d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whole code in one place"
      ],
      "metadata": {
        "id": "A4NEbTAjRhGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "\n",
        "\n",
        "# from keras.layers import Input, LSTM, Dense, TimeDistributed, Activation, BatchNormalization, Dropout, Bidirectional\n",
        "# from keras.models import Sequential\n",
        "# from keras.utils import Sequence\n",
        "# from keras.layers import CuDNNLSTM\n",
        "\n",
        "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
        "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\n",
        "EPOCHS=10\n",
        "BATCH_SIZE=64\n",
        "NAME= f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
        "\n",
        "\n",
        "def classify(current, future):\n",
        "    if float(future) > float(current):  # if the future price is higher than the current, that's a buy, or a 1\n",
        "        return 1\n",
        "    else:  # otherwise... it's a 0!\n",
        "        return 0\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic. Those nasty NaNs love to creep in.\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "    print(ratio)\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/crypto_data/LTC-USD.csv',names=['time','low','high','open','close','volume'])\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "#print(main_df.head())\n",
        "\n",
        "times = sorted(main_df.index.values)  # get the times\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
        "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
        "\n",
        "    for col in df.columns:  # go through all of the columns\n",
        "        if col != \"target\":  # normalize all ... except for the target itself!\n",
        "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
        "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
        "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
        "\n",
        "    df.dropna(inplace=True)  # cleanup again... jic.\n",
        "\n",
        "\n",
        "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
        "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
        "\n",
        "    for i in df.values:  # iterate over the values\n",
        "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
        "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
        "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
        "\n",
        "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
        "\n",
        "    buys = []  # list that will store our buy sequences and targets\n",
        "    sells = []  # list that will store our sell sequences and targets\n",
        "\n",
        "    for seq, target in sequential_data:  # iterate over the sequential data\n",
        "        if target == 0:  # if it's a \"not buy\"\n",
        "            sells.append([seq, target])  # append to sells list\n",
        "        elif target == 1:  # otherwise if the target is a 1...\n",
        "            buys.append([seq, target])  # it's a buy!\n",
        "\n",
        "    random.shuffle(buys)  # shuffle the buys\n",
        "    random.shuffle(sells)  # shuffle the sells!\n",
        "\n",
        "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
        "\n",
        "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
        "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
        "\n",
        "    sequential_data = buys+sells  # add them together\n",
        "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for seq, target in sequential_data:  # going over our new sequential data\n",
        "        X.append(seq)  # X is the sequences\n",
        "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
        "\n",
        "    return np.array(X), y  # return X and y...and make X a numpy array!\n",
        "\n",
        "\n",
        "main_df = pd.DataFrame() # begin empty\n",
        "\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\n",
        "for ratio in ratios:  # begin iteration\n",
        "\n",
        "    ratio = ratio.split('.csv')[0]  # split away the ticker from the file-name\n",
        "    dataset = f'training_datas/{ratio}.csv'  # get the full path to the file.\n",
        "    df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/crypto_data/LTC-USD.csv',names=['time','low','high','open','close','volume'])\n",
        "\n",
        "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
        "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
        "\n",
        "    df.set_index(\"time\", inplace=True)  # set time as index so we can join them on this shared time\n",
        "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
        "\n",
        "    if len(main_df)==0:  # if the dataframe is empty\n",
        "        main_df = df  # then it's just the current df\n",
        "    else:  # otherwise, join this data to the main one\n",
        "        main_df = main_df.join(df)\n",
        "\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "main_df.dropna(inplace=True)\n",
        "#print(main_df.head())  # how did we do??\n",
        "\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
        "\n",
        "main_df.dropna(inplace=True)\n",
        "\n",
        "## here, split away some slice of the future data from the main main_df.\n",
        "times = sorted(main_df.index.values)\n",
        "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]\n",
        "\n",
        "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
        "main_df = main_df[(main_df.index < last_5pct)]\n",
        "\n",
        "\n",
        "\n",
        "train_x, train_y = preprocess_df(main_df)\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\n",
        "\n",
        "\n",
        "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
        "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
        "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "# Compile model\n",
        "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# TensorBoard callback:\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
        "\n",
        "# ModelCheckpoint callback\n",
        "\n",
        "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
        "checkpoint = ModelCheckpoint(\"crypto_models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
        "\n",
        "\n",
        "train_x = np.asarray(train_x)\n",
        "train_y = np.asarray(train_y)\n",
        "validation_x = np.asarray(validation_x)\n",
        "validation_y = np.asarray(validation_y)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(validation_x, validation_y),\n",
        "    callbacks=[tensorboard],\n",
        "    # callbacks=[tensorboard, checkpoint],\n",
        ")\n",
        "\n",
        "\n",
        "# Score model\n",
        "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save model\n",
        "model.save(\"crypto_models/{}\".format(NAME))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyiVmrji7FJS",
        "outputId": "51590d42-4689-4c10-bd77-7b9aeeb41ff8"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTC-USD\n",
            "LTC-USD\n",
            "BCH-USD\n",
            "ETH-USD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data: 81638 validation: 4022\n",
            "Dont buys: 40819, buys: 40819\n",
            "VALIDATION Dont buys: 2011, buys: 2011\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1276/1276 [==============================] - 27s 18ms/step - loss: 0.7138 - accuracy: 0.5062 - val_loss: 0.6931 - val_accuracy: 0.5022\n",
            "Epoch 2/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6939 - accuracy: 0.5010 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6934 - accuracy: 0.5034 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "1276/1276 [==============================] - 23s 18ms/step - loss: 0.6934 - accuracy: 0.5029 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "1276/1276 [==============================] - 22s 18ms/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6932 - val_accuracy: 0.5127\n",
            "Epoch 6/10\n",
            "1276/1276 [==============================] - 22s 18ms/step - loss: 0.6932 - accuracy: 0.5083 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6932 - accuracy: 0.5011 - val_loss: 0.6940 - val_accuracy: 0.4973\n",
            "Epoch 8/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6930 - val_accuracy: 0.5129\n",
            "Epoch 9/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6930 - accuracy: 0.5041 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "1276/1276 [==============================] - 22s 17ms/step - loss: 0.6915 - accuracy: 0.5254 - val_loss: 0.6902 - val_accuracy: 0.5229\n",
            "Test loss: 0.6901781558990479\n",
            "Test accuracy: 0.5228741765022278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: crypto_models/60-SEQ-3-PRED-1655191943/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: crypto_models/60-SEQ-3-PRED-1655191943/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f4725d36fd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f46bbd16150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f46bb45db90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=logs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wK0t2SQ_KFl",
        "outputId": "fded81ca-a2e1-4c11-e5ce-6c13c9ca879f"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/util.py\", line 320, in _exit_function\n",
            "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}