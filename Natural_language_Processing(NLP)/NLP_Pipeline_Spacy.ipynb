{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Pipeline_Spacy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZIJa7kFq8TsJ"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install en_core_web_sm"
      ],
      "metadata": {
        "id": "msSjw90H888d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp= spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-oGdy3O8bgX",
        "outputId": "27fe02cf-d3fd-4c72-936f-06e727df88de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain\n",
            "america\n",
            "ate\n",
            "100\n",
            "$\n",
            "of\n",
            "samosa\n",
            ".\n",
            "Then\n",
            "he\n",
            "said\n",
            "I\n",
            "can\n",
            "do\n",
            "this\n",
            "all\n",
            "day\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQg1U8_X8m5F",
        "outputId": "df56a052-e5f9-4b6f-ea46-69e33f163e8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install en_core_web_sm. After install this we are using/loading this package now in spacy.\n",
        "\n",
        "nlp= spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "ZFk5Yw5B8uGs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> These are all the inbuild features that we are getting when we are loading a pre-defined trained pipeline(**en_core_web_sm**)."
      ],
      "metadata": {
        "id": "Bxbgt-FS_bH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csNb8u7O9mzj",
        "outputId": "ac008a9f-aba5-4ae5-93ff-afb0e11d647a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds_5Obzg99yJ",
        "outputId": "88435ac0-9628-4fdd-9a21-aa6ddbc044b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f4795480fa0>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f4795480f30>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f479531b850>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f4795251460>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f4795258410>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f479531b6d0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">'pos' -> Part of Speech, 'lemma' is the base word.\n",
        "\n",
        "> Part of Speech is coming from the 'tagger' component of the pipeline.\n",
        "\n",
        "> Lemma is icmung from the 'lemmatizer' component of the pipeline."
      ],
      "metadata": {
        "id": "qAyt47Mu_IkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.pos_, \" | \", token.lemma_) # 'pos' -> Part of Speech, 'lemma' is the base word."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxKXWABu-BaD",
        "outputId": "fc418665-28ff-4aac-fc04-acc5d8f0f1a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |  PROPN  |  Captain\n",
            "america  |  PROPN  |  america\n",
            "ate  |  VERB  |  eat\n",
            "100  |  NUM  |  100\n",
            "$  |  NUM  |  $\n",
            "of  |  ADP  |  of\n",
            "samosa  |  NOUN  |  samosa\n",
            ".  |  PUNCT  |  .\n",
            "Then  |  ADV  |  then\n",
            "he  |  PRON  |  he\n",
            "said  |  VERB  |  say\n",
            "I  |  PRON  |  I\n",
            "can  |  AUX  |  can\n",
            "do  |  VERB  |  do\n",
            "this  |  PRON  |  this\n",
            "all  |  DET  |  all\n",
            "day  |  NOUN  |  day\n",
            ".  |  PUNCT  |  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition(NER)"
      ],
      "metadata": {
        "id": "yOJ1V0_kA8Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PzLVPq0AjPY",
        "outputId": "a9f0b298-e4e4-4b92-a4f6-f8f97cbc6af4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display it in a fancier way"
      ],
      "metadata": {
        "id": "htQYxUUg-RPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "z86V9YMFAMHf",
        "outputId": "806cfed4-8d8c-471a-bfb8-4744e069cf37"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    Tesla Inc\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n is going to acquire twitter for \\n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    $45 billion\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\\n</mark>\\n</div>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blank Pipeline with Custom Components\n",
        "\n",
        "\n",
        "> If we want custom components in a blank pipeline, then we have to follow the below code. Since I don't want the in-build components that comes with loading a pre-build/trained pipeline.\n",
        "\n",
        "> The problem this is solving is that someone wants to use only one of the components and not all of them.\n"
      ],
      "metadata": {
        "id": "1Zjm6w9AARqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp= spacy.blank(\"en\")\n",
        "\n",
        "nlp.add_pipe(\"ner\", source=source_nlp_pipeline) # From the 'en_core_web_sm' pipeline, add only 'ner' component here.\n",
        "nlp.pipe_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HalWuk4hCQJ7",
        "outputId": "33049049-c75c-49ad-9e5e-ebda93887bb9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsUsc_DCDM-e",
        "outputId": "c010f04a-1f43-458d-a619-c6cb5ae02f5a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f4794fa8d50>)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpFPIVr-DUw9",
        "outputId": "59613867-4ff5-4d93-fe64-c85afb0e158a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jZtkpNTHDje1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}