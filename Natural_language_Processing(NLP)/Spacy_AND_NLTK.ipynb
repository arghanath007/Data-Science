{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy_AND_NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy VS NLTK"
      ],
      "metadata": {
        "id": "CWPSCay6kqdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-4Yz-axjkpd"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "YrSU7K6ijqdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm  # Downloading all the packages related to English Language"
      ],
      "metadata": {
        "id": "PVDMKTmXjv3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Spacy\n",
        "\n",
        "> Sentence and Word Tokenization using Spacy\n",
        "\n",
        "> Using Object Oriented Programming Approach\n",
        "\n",
        "> This out of the box gives us the best algorithm, best option and it works."
      ],
      "metadata": {
        "id": "85bam17kj8Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "VpB9ENT9kpoE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For splitting senteces"
      ],
      "metadata": {
        "id": "FMe2-Zb-l4aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_spacy=spacy.load('en_core_web_sm') # Load/start using this package here('en_core_web_sm').\n",
        "nlp_spacy\n",
        "\n",
        "doc= nlp_spacy(\"Dr. Strange loves pav bhaji and etc. of mumbai. Hulk loves chaat of delhi\")\n",
        "\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGPk2Ow6kvEK",
        "outputId": "15a74897-0d9f-45d1-fbc9-08d1e08695c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. Strange loves pav bhaji and etc. of mumbai.\n",
            "Hulk loves chaat of delhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For splitting words"
      ],
      "metadata": {
        "id": "tOysre3Nl8V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc.sents:\n",
        "  for word in sentence:\n",
        "    print(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTZWku4Yk2sV",
        "outputId": "35fb6ca3-5dac-4d94-a091-2d2385eed19f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "Strange\n",
            "loves\n",
            "pav\n",
            "bhaji\n",
            "and\n",
            "etc\n",
            ".\n",
            "of\n",
            "mumbai\n",
            ".\n",
            "Hulk\n",
            "loves\n",
            "chaat\n",
            "of\n",
            "delhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK\n",
        "\n",
        "> String Processing Library\n",
        "\n",
        "> It is like a DSLR manual camera, which we need to tweak to get the full potentail of this powerful library."
      ],
      "metadata": {
        "id": "W8Rk1hfSl_r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cluKb6zBmeNQ",
        "outputId": "e2287e12-25ca-4410-cef1-01bc535a3997"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Tokenization"
      ],
      "metadata": {
        "id": "89WU9hVGn6G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_tokenize(\"Dr. Strange loves pav bhaji and etc. of mumbai. Hulk loves chaat of delhi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB5VH24PmgN6",
        "outputId": "005e18dc-9c8b-42c8-d040-47a9a44e1d90"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr.',\n",
              " 'Strange loves pav bhaji and etc.',\n",
              " 'of mumbai.',\n",
              " 'Hulk loves chaat of delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Tokenization"
      ],
      "metadata": {
        "id": "qYPRUW9Mn8_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokenize(\"Dr. Strange loves pav bhaji and etc. of mumbai. Hulk loves chaat of delhi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp-4fz1gnORd",
        "outputId": "202d757a-cb8d-4cad-8698-cd9dcc61220b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr',\n",
              " '.',\n",
              " 'Strange',\n",
              " 'loves',\n",
              " 'pav',\n",
              " 'bhaji',\n",
              " 'and',\n",
              " 'etc',\n",
              " '.',\n",
              " 'of',\n",
              " 'mumbai',\n",
              " '.',\n",
              " 'Hulk',\n",
              " 'loves',\n",
              " 'chaat',\n",
              " 'of',\n",
              " 'delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_2K-FLE4oC2X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}